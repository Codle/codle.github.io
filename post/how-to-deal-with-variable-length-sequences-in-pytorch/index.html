<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.37.1" />
  <meta name="author" content="YIN. Zheng-Qiang">

  
  
  
  
    
      
    
  
  <meta name="description" content="最近在做文本情感分析方面的研究，主要使用了 PyTorch 来做实验。在针对文本的处理过程中，遇见了文本长度不唯一的问题，在此主要讲 PyTorch 中的解决方法。
写作时的系统环境：
 PyTorch ：0.3.1 Python ：3.6.2  问题来源 情感分析采用神经网络的方法和文本分类比较类似。其工作流程为输入文本，输出情感类别。对比文本分类和图片分类，图片在进行处理前都会将图片缩放为尺寸相当，这样保证了输入与输出的尺寸在一开始就被确定。而文本，一般以句子为一个输入，每个词对应一个 LSTM 或者 RNN 单元，然而当多个样本进行训练时，句子长度不一给模型训练带来了极大麻烦。
解决方法 解决思路 通常情况下，我们会考虑使用 padding 的方法来统一序列长度。例如，现在有两句话：“Look at the cat”和 “OK&rdquo;，第一句话的长度为 4，第二句为 1，那么我们就考虑将第二句话后面补 0 使得其长度达到 4。此时，句子长度便统一了。
这种方法有个明显的问题：如果训练集存在一个非常长的句子，而大部分句子都非常短小，那么整个训练集合都会补充相当数量的 0。此时再进行神经网络训练的时候，输入矩阵非常稀疏，会导致训练效果和训练效率都变得很低。
为了解决这个问题，PyTorch 中采用了一种打包解包的方法。
如图所示，将填充后的数据进行打包（重新压缩），data 存储拼接后的字符串，batch_sizes 保存原始串的长度。其实就是采用了一个结构体，一部分存储全部的数据，一部分存储每个串的长度，然后根据串长度取回原来的序列，只是 PyTorch 中实现了许多我们可以用到的方法，而不用自己手动实现了。
PyTorch 实现 在 Pytorch 中，提供了两个方法torch.nn.utils.rnn.pack_padded_sequence()和torch.nn.utils.rnn.pad_packed_sequence() （以下简写为 pack_padded_sequence() 和 pad_packed_sequence() 。
pack_padded_sequence 文档 原型：
torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)  打包一个填充过的可变长度序列的 Variable 。
输入应该是 TxBx* ，其中 T 是最长的序列的长度（等同于lengths[0]），B 是 batch 大小，而 * 可以是任意数字的维度 （包括 0）。如果 batch_first 为 True ，那么输入就应该是 BxTx* 。">

  
  <link rel="alternate" hreflang="en-us" href="https://codle.net/post/how-to-deal-with-variable-length-sequences-in-pytorch/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.1/css/academicons.min.css" integrity="sha512-NThgw3XKQ1absAahW6to7Ey42uycrVvfNfyjqcFNgCmOCQ5AR4AO0SiXrN+8ZtYeappp56lk1WtvjVmEa+VR6A==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="https://codle.net/index.xml" type="application/rss+xml" title="Codle">
  <link rel="feed" href="https://codle.net/index.xml" type="application/rss+xml" title="Codle">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://codle.net/post/how-to-deal-with-variable-length-sequences-in-pytorch/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@GeorgeCushen">
  <meta property="twitter:creator" content="@GeorgeCushen">
  
  <meta property="og:site_name" content="Codle">
  <meta property="og:url" content="https://codle.net/post/how-to-deal-with-variable-length-sequences-in-pytorch/">
  <meta property="og:title" content="PyTorch 中可变长序列的处理 | Codle">
  <meta property="og:description" content="最近在做文本情感分析方面的研究，主要使用了 PyTorch 来做实验。在针对文本的处理过程中，遇见了文本长度不唯一的问题，在此主要讲 PyTorch 中的解决方法。
写作时的系统环境：
 PyTorch ：0.3.1 Python ：3.6.2  问题来源 情感分析采用神经网络的方法和文本分类比较类似。其工作流程为输入文本，输出情感类别。对比文本分类和图片分类，图片在进行处理前都会将图片缩放为尺寸相当，这样保证了输入与输出的尺寸在一开始就被确定。而文本，一般以句子为一个输入，每个词对应一个 LSTM 或者 RNN 单元，然而当多个样本进行训练时，句子长度不一给模型训练带来了极大麻烦。
解决方法 解决思路 通常情况下，我们会考虑使用 padding 的方法来统一序列长度。例如，现在有两句话：“Look at the cat”和 “OK&rdquo;，第一句话的长度为 4，第二句为 1，那么我们就考虑将第二句话后面补 0 使得其长度达到 4。此时，句子长度便统一了。
这种方法有个明显的问题：如果训练集存在一个非常长的句子，而大部分句子都非常短小，那么整个训练集合都会补充相当数量的 0。此时再进行神经网络训练的时候，输入矩阵非常稀疏，会导致训练效果和训练效率都变得很低。
为了解决这个问题，PyTorch 中采用了一种打包解包的方法。
如图所示，将填充后的数据进行打包（重新压缩），data 存储拼接后的字符串，batch_sizes 保存原始串的长度。其实就是采用了一个结构体，一部分存储全部的数据，一部分存储每个串的长度，然后根据串长度取回原来的序列，只是 PyTorch 中实现了许多我们可以用到的方法，而不用自己手动实现了。
PyTorch 实现 在 Pytorch 中，提供了两个方法torch.nn.utils.rnn.pack_padded_sequence()和torch.nn.utils.rnn.pad_packed_sequence() （以下简写为 pack_padded_sequence() 和 pad_packed_sequence() 。
pack_padded_sequence 文档 原型：
torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)  打包一个填充过的可变长度序列的 Variable 。
输入应该是 TxBx* ，其中 T 是最长的序列的长度（等同于lengths[0]），B 是 batch 大小，而 * 可以是任意数字的维度 （包括 0）。如果 batch_first 为 True ，那么输入就应该是 BxTx* 。">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-03-16T16:21:08&#43;08:00">
  
  <meta property="article:modified_time" content="2018-03-16T16:21:08&#43;08:00">
  

  

  <title>PyTorch 中可变长序列的处理 | Codle</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">切换导航</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Codle</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>首页</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>文章</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>项目</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>联系</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">PyTorch 中可变长序列的处理</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-03-16 16:21:08 &#43;0800 CST" itemprop="datePublished dateModified">
      Mar 16, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="YIN. Zheng-Qiang">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    2 分钟阅读时间
  </span>
  

  
  
  <span class="middot-divider"></span>
  <a href="https://codle.net/post/how-to-deal-with-variable-length-sequences-in-pytorch/#disqus_thread"></a>
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/categories/pytorch">PyTorch</a
    >
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=PyTorch%20%e4%b8%ad%e5%8f%af%e5%8f%98%e9%95%bf%e5%ba%8f%e5%88%97%e7%9a%84%e5%a4%84%e7%90%86&amp;url=https%3a%2f%2fcodle.net%2fpost%2fhow-to-deal-with-variable-length-sequences-in-pytorch%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fcodle.net%2fpost%2fhow-to-deal-with-variable-length-sequences-in-pytorch%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcodle.net%2fpost%2fhow-to-deal-with-variable-length-sequences-in-pytorch%2f&amp;title=PyTorch%20%e4%b8%ad%e5%8f%af%e5%8f%98%e9%95%bf%e5%ba%8f%e5%88%97%e7%9a%84%e5%a4%84%e7%90%86"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fcodle.net%2fpost%2fhow-to-deal-with-variable-length-sequences-in-pytorch%2f&amp;title=PyTorch%20%e4%b8%ad%e5%8f%af%e5%8f%98%e9%95%bf%e5%ba%8f%e5%88%97%e7%9a%84%e5%a4%84%e7%90%86"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=PyTorch%20%e4%b8%ad%e5%8f%af%e5%8f%98%e9%95%bf%e5%ba%8f%e5%88%97%e7%9a%84%e5%a4%84%e7%90%86&amp;body=https%3a%2f%2fcodle.net%2fpost%2fhow-to-deal-with-variable-length-sequences-in-pytorch%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p>最近在做文本情感分析方面的研究，主要使用了 PyTorch 来做实验。在针对文本的处理过程中，遇见了文本长度不唯一的问题，在此主要讲 PyTorch 中的解决方法。</p>

<p>写作时的系统环境：</p>

<ul>
<li>PyTorch ：0.3.1</li>
<li>Python ：3.6.2</li>
</ul>

<h2 id="问题来源">问题来源</h2>

<p>情感分析采用神经网络的方法和文本分类比较类似。其工作流程为输入文本，输出情感类别。对比文本分类和图片分类，图片在进行处理前都会将图片缩放为尺寸相当，这样保证了输入与输出的尺寸在一开始就被确定。而文本，一般以句子为一个输入，每个词对应一个 LSTM 或者 RNN 单元，然而当多个样本进行训练时，句子长度不一给模型训练带来了极大麻烦。</p>

<h2 id="解决方法">解决方法</h2>

<h3 id="解决思路">解决思路</h3>

<p>通常情况下，我们会考虑使用 padding 的方法来统一序列长度。例如，现在有两句话：“Look at the cat”和 “OK&rdquo;，第一句话的长度为 4，第二句为 1，那么我们就考虑将第二句话后面补 0 使得其长度达到 4。此时，句子长度便统一了。</p>

<p>这种方法有个明显的问题：如果训练集存在一个非常长的句子，而大部分句子都非常短小，那么整个训练集合都会补充相当数量的 0。此时再进行神经网络训练的时候，输入矩阵非常稀疏，会导致训练效果和训练效率都变得很低。</p>

<p>为了解决这个问题，PyTorch 中采用了一种打包解包的方法。</p>

<p><img src="http://upload-images.jianshu.io/upload_images/2397007-93b28bb8d6948571.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="" /></p>

<p>如图所示，将填充后的数据进行打包（重新压缩），data 存储拼接后的字符串，batch_sizes 保存原始串的长度。其实就是采用了一个结构体，一部分存储全部的数据，一部分存储每个串的长度，然后根据串长度取回原来的序列，只是 PyTorch 中实现了许多我们可以用到的方法，而不用自己手动实现了。</p>

<h3 id="pytorch-实现">PyTorch 实现</h3>

<p>在 Pytorch 中，提供了两个方法<code>torch.nn.utils.rnn.pack_padded_sequence()</code>和<code>torch.nn.utils.rnn.pad_packed_sequence()</code> （以下简写为 <code>pack_padded_sequence()</code> 和 <code>pad_packed_sequence()</code> 。</p>

<h4 id="pack-padded-sequence-文档">pack_padded_sequence 文档</h4>

<p>原型：</p>

<pre><code class="language-python">torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)
</code></pre>

<p>打包一个填充过的可变长度序列的 Variable 。</p>

<p>输入应该是 TxBx* ，其中 T 是最长的序列的长度（等同于lengths[0]），B 是 batch 大小，而 * 可以是任意数字的维度 （包括 0）。如果 batch_first 为 True ，那么输入就应该是 BxTx* 。</p>

<p>序列应该被按照序列的长度呈降序排列。例如： input[:,0] 应该是最长的序列，而且 input[:,B-1] 是最短的。</p>

<p>参数：</p>

<ul>
<li>input (Variable) – 经过填充的批量可变长度序列。</li>
<li>lengths (list[int]) – 储存当前批次中的每个可变长度序列的长度的 list 对象。</li>
<li>batch_first (bool, optional) – 如果为 True，输入则应该是 BxTx* 格式。</li>
</ul>

<p>返回值:</p>

<ul>
<li>一个 PackedSequence 对象</li>
</ul>

<h4 id="pad-packed-sequence-文档">pad_packed_sequence 文档</h4>

<p>原型：</p>

<pre><code class="language-python">torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0)
</code></pre>

<p>填充一批打包后的可变长度序列。</p>

<p>是 pack_padded_sequence() 的逆操作。</p>

<p>返回值 Variable 的数据格式为 TxBx* ，其中 T 是最长序列的长度， B 是 batch 大小。如果 batch_first 为 True，数据会被转为 BxTx* 格式。</p>

<p>当前批次中的元素会被按照长度进行排序。</p>

<p>参数:</p>

<ul>
<li>sequence (PackedSequence) – 准备填充的批量元素。</li>
<li>batch_first (bool, optional) – 如果为 True，输出会是 BxTx* 格式。</li>
<li>padding_value (float, optional) – 填充的元素。</li>
</ul>

<p>返回值:</p>

<ul>
<li>填充序列的 Variable 元组以及包含当前批次中序列长度的 list 对象。</li>
</ul>

<h3 id="使用方法">使用方法</h3>

<p>从文档中可以看出，当我们将数据填充整齐后，可以使用<code>pack_padded_sequence()</code> 和<code>pad_packed_sequence()</code> 方法可以将数据进行打包和解包（填充这一步暂且需要自己手动做）。</p>

<p>打包后得到 PackedSequence 对象，其主要由两个部分组成：</p>

<ul>
<li>所有序列的数据</li>
<li>每个序列的原始长度</li>
</ul>

<p>针对 PackedSequence 对象，我们可以使用很多方法而不需要去解包（解包这一步会比较慢）。PackedSequence 对象可以被当作输入用在 PyTorch 中的大部分模块中。</p>

<h3 id="尚未实装的方法">尚未实装的方法</h3>

<p>本方法来源于 <a href="https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120/24" target="_blank">PyTorch 官方论坛</a> ，因为所用版本还未发布所以没有测试。文中指出在 PyTorch 0.4 版本中增加了新的方法来实现 Padding 和 Packing。</p>

<p>序列打包：</p>

<pre><code class="language-python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch.nn.utils.rnn as rnn_utils
&gt;&gt;&gt; a = torch.Tensor([1, 2, 3])
&gt;&gt;&gt; b = torch.Tensor([4, 5])
&gt;&gt;&gt; c = torch.Tensor([6])
&gt;&gt;&gt; packed = rnn_utils.pack_sequence([a, b, c])
</code></pre>

<p>序列解包：</p>

<pre><code class="language-python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch.nn.utils.rnn as rnn_utils
&gt;&gt;&gt; a = torch.Tensor([1, 2, 3])
&gt;&gt;&gt; b = torch.Tensor([4, 5])
&gt;&gt;&gt; c = torch.Tensor([6])
&gt;&gt;&gt; rnn_utils.pad_sequence([a, b, c], batch_first=True)

 1  2  3
 4  5  0
 6  0  0
[torch.FloatTensor of size (3,3)]
</code></pre>

<h2 id="总结">总结</h2>

<p>在 PyTorch 中处理可变长度序列的基本流程：</p>

<ol>
<li>将序列填充，并按照原始长度降序排列；</li>
<li>使用 <code>pack_padded_sequence()</code> 对序列进行打包；</li>
<li>使用 PackedSequence 对象继续编写程序。</li>
<li>（可选）如果需要还原，则使用 <code>pad_packed_sequence()</code> 方法。</li>
</ol>

<h2 id="参考">参考</h2>

<ul>
<li><a href="https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/" target="_blank">Simple working example how to use packing for variable-length sequence inputs for rnn, <em>PyTorch Discuss</em></a></li>
<li><a href="https://zhuanlan.zhihu.com/p/34418001" target="_blank">忆臻, pytorch中如何处理RNN输入变长序列padding, <em>知乎</em></a></li>
<li><a href="http://www.cnblogs.com/lindaxin/p/8052043.html" target="_blank">深度学习1, pytorch对可变长度序列的处理, <em>博客园</em></a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28472545" target="_blank">赵普, pytorch RNN 变长输入 padding, <em>知乎</em></a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzAwNDI4ODcxNA==&amp;mid=2652245658&amp;idx=1&amp;sn=03c63ad0e0657e5272ea2e3c7e346521&amp;scene=0#wechat_redirect" target="_blank">Thomas Wolf, 如何用pyTorch改造基于Keras的MIT情感理解模型, <em>人工智能头条</em></a></li>
</ul>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/pytorch">PyTorch</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/nlp">NLP</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>相关</h3>
      <ul>
        
        <li><a href="/project/sentiment-ananalysis/">面向中文文本的情感倾向分析</a></li>
        
      </ul>
    </div>
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script>
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "codle" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 蜀ICP备17002933号-2 &middot; 

      Powered by the
      <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">引用</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> 复制
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> 下载
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script id="dsq-count-scr" src="//codle.disqus.com/count.js" async></script>
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>

