<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Codle</title>
    <link>https://codle.net/post/</link>
    <description>Recent content in Posts on Codle</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&amp;copy; 2018 蜀ICP备17002933号-2</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://codle.net/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>推荐读物</title>
      <link>https://codle.net/post/recommend-books/</link>
      <pubDate>Sat, 17 Mar 2018 18:06:01 +0800</pubDate>
      
      <guid>https://codle.net/post/recommend-books/</guid>
      <description>Python 纯入门向推荐「简明 Python 教程」， 英文名「A Byte of Python」，是一本英文读物。
 在线阅读：A Byte of Python  廖雪峰的 Python3 教程。在他的个人博客上，不仅介绍了基本的语法也有一些使用 Python 写的有趣的小项目。
 网站地址：Python3  Python 工具书籍类书籍：「Python Cookbook」，和「C++ Prime」类似，介绍了 Python 中的基本算法写法、模块写法等。内容太多，用到的时候查询即可。
 下载地址：https://pan.baidu.com/s/1pL1cI9d  Machine Learning 吴恩达在斯坦福时期教授的课程的在线版本，也是 Coursera 平台起步阶段最热门的课程。每节课都有小作业，可以在线评分。数学代码使用使用 Ocatve ，一款开源的数学软件，和 Matlab 代码可以通用，也就是说使用 Matlab 或者 Ocatve 都可以完成作业。
课程相比于斯坦福的本科生版本有所删减，难度降低了，追求更高层次的可以去网易公开课看本科教室录制的斯坦福 cs229 。
课程地址：旁听免费
除了 Coursera 平台的机器学习课程，台湾大学的林轩田教授的两门课程也是非常值得一看的，且全部使用的国语讲述。
课程地址： + 机器学习基石(B站源) + 机器学习技法(B站源)
Deep Learning 吴恩达创业项目 deeplearning.ai 的课程，较短偏向于工程应用，很多公式都没有推导，适合入门，使用 TensorFlow 作为深度学习框架来布置作业。分为五个课程：
 Neural Networks and Deep Learning Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization Structuring Machine Learning Projects Convolutional Neural Networks Sequence Models  课程地址：旁听免费</description>
    </item>
    
    <item>
      <title>PyTorch 中可变长序列的处理</title>
      <link>https://codle.net/post/how-to-deal-with-variable-length-sequences-in-pytorch/</link>
      <pubDate>Fri, 16 Mar 2018 16:21:08 +0800</pubDate>
      
      <guid>https://codle.net/post/how-to-deal-with-variable-length-sequences-in-pytorch/</guid>
      <description>最近在做文本情感分析方面的研究，主要使用了 PyTorch 来做实验。在针对文本的处理过程中，遇见了文本长度不唯一的问题，在此主要讲 PyTorch 中的解决方法。
写作时的系统环境：
 PyTorch ：0.3.1 Python ：3.6.2  问题来源 情感分析采用神经网络的方法和文本分类比较类似。其工作流程为输入文本，输出情感类别。对比文本分类和图片分类，图片在进行处理前都会将图片缩放为尺寸相当，这样保证了输入与输出的尺寸在一开始就被确定。而文本，一般以句子为一个输入，每个词对应一个 LSTM 或者 RNN 单元，然而当多个样本进行训练时，句子长度不一给模型训练带来了极大麻烦。
解决方法 解决思路 通常情况下，我们会考虑使用 padding 的方法来统一序列长度。例如，现在有两句话：“Look at the cat”和 “OK&amp;rdquo;，第一句话的长度为 4，第二句为 1，那么我们就考虑将第二句话后面补 0 使得其长度达到 4。此时，句子长度便统一了。
这种方法有个明显的问题：如果训练集存在一个非常长的句子，而大部分句子都非常短小，那么整个训练集合都会补充相当数量的 0。此时再进行神经网络训练的时候，输入矩阵非常稀疏，会导致训练效果和训练效率都变得很低。
为了解决这个问题，PyTorch 中采用了一种打包解包的方法。
如图所示，将填充后的数据进行打包（重新压缩），data 存储拼接后的字符串，batch_sizes 保存原始串的长度。其实就是采用了一个结构体，一部分存储全部的数据，一部分存储每个串的长度，然后根据串长度取回原来的序列，只是 PyTorch 中实现了许多我们可以用到的方法，而不用自己手动实现了。
PyTorch 实现 在 Pytorch 中，提供了两个方法torch.nn.utils.rnn.pack_padded_sequence()和torch.nn.utils.rnn.pad_packed_sequence() （以下简写为 pack_padded_sequence() 和 pad_packed_sequence() 。
pack_padded_sequence 文档 原型：
torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)  打包一个填充过的可变长度序列的 Variable 。
输入应该是 TxBx* ，其中 T 是最长的序列的长度（等同于lengths[0]），B 是 batch 大小，而 * 可以是任意数字的维度 （包括 0）。如果 batch_first 为 True ，那么输入就应该是 BxTx* 。</description>
    </item>
    
    <item>
      <title>PyTorch 基础入门（一）</title>
      <link>https://codle.net/post/pytorch-start-tutorial-1/</link>
      <pubDate>Mon, 11 Dec 2017 16:48:54 +0800</pubDate>
      
      <guid>https://codle.net/post/pytorch-start-tutorial-1/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;本文参考自deeplearning.ai的在线课程deep learning specialization课程中的第二项课程中的Tensorflow教程，希望能够对使用Pytorch来进行研究深度学习的同学有所帮助。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>