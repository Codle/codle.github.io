<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Codle on Codle</title>
    <link>https://codle.net/</link>
    <description>Recent content in Codle on Codle</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&amp;copy; 2018 蜀ICP备17002933号-2</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Pytorch 基础入门（一）</title>
      <link>https://codle.net/post/pytorch-start-tutorial-1/</link>
      <pubDate>Mon, 11 Dec 2017 16:48:54 +0800</pubDate>
      
      <guid>https://codle.net/post/pytorch-start-tutorial-1/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;本文参考自deeplearning.ai的在线课程deep learning specialization课程中的第二项课程中的Tensorflow教程，希望能够对使用Pytorch来进行研究深度学习的同学有所帮助。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;采用 Jupyter Notebook 的方式编写本文。你可以在相关材料中找到相应的文件来尝试自己练习。通过本文，将会涉及到：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;初始化变量&lt;/li&gt;
&lt;li&gt;创建模型&lt;/li&gt;
&lt;li&gt;训练算法&lt;/li&gt;
&lt;li&gt;生成一个神经网络&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;pytorch-库&#34;&gt;Pytorch 库&lt;/h1&gt;

&lt;p&gt;首先，我们引入相关的库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math
import numpy as np
import matplotlib.pyplot as plt
import torch

%matplotlib inline
np.random.seed(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;导入库后，我们将在接下来的不同的示例中去学习使用他们。&lt;/p&gt;

&lt;h1 id=&#34;tensor&#34;&gt;Tensor&lt;/h1&gt;

&lt;p&gt;在一切开始之前，我们需要学习在 Pytorch 中最基本的数据单元 Tensor。如果之前对 numpy 有所使用，那么我们可以先简单的认为 Tensor 和 narray 是类似的数据类型，即多维的数据单元。&lt;/p&gt;

&lt;p&gt;Tensor 有多种创建方式，下面简单介绍几种：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 直接创建未初始化的 Tensor 变量，参数为创建的矩阵维数
X = torch.Tensor(5, 3)

# 指明为浮点型的 Tensor 变量
X_float = torch.FloatTensor(5, 3)

# 列表创建
# 一维
X_one = torch.Tensor([1, 2, 3])
# 二维
X_two = torch.Tensor([[1, 2, 3], [4, 5, 6]])

# 使用 numpy 变量进行创建
nones = np.ones((5, 3))
X_ones = torch.Tensor(nones)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;示例-损失函数&#34;&gt;示例：损失函数&lt;/h2&gt;

&lt;p&gt;我们先将计算损失函数作为本教程的第一个示例：
$$
loss = L(\hat y, y)=(\hat y^{(i)}-y^{(i)})^2
$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y_hat = torch.Tensor([1, 1, 0])
y = torch.Tensor([1, 0, 1])
loss = (y_hat - y)**2
print(loss)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Out:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0 
1 
1
[torch.FloatTensor of size 3]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以使用 &lt;code&gt;torch.mean()&lt;/code&gt; 来计算均方差：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;loss = torch.mean((y_hat - y)**2)
print(loss)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Out:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0.6666666666666666
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;variable&#34;&gt;Variable&lt;/h1&gt;

&lt;p&gt;前面主要讲了最基本的数据类型 Tensor ，这一节主要谈论 Variable 类型，这一类型主要用于存放变化的数据。如何理解 Variable 变量？在使用框架之前我们曾使用 numpy 来编写简单的神经网络，numpy 单一的数据类型已经能够满足编写神经网络了，那么为什么要用 Variable 呢？&lt;/p&gt;

&lt;p&gt;Variable 类型最为主要的功能就是计算变量导数，对于一个线性函数：
$$
y = 2x+3
$$
$x$ 和 $y$ 被称为变量（自变量和因变量）。$x$ 变化时，$y$ 随 $x$ 的变化而变化，那么当 $x$ 变化 0.1的时候， $y$ 变化多少呢，在通常情况下，我们可以通过计算导数：
$$
\frac{dy}{dx}=2,\delta y=\frac{dy}{dx}\times\delta x=2\times \delta x=2\times0.1=0.2
$$
在之前的学习中，我们自己手动计算导数，将其放入神经网络中，而在 Pytorch 框架中这一计算不再需要我们自己动手了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.autograd import Variable
tensor = torch.Tensor(1)
variable = Variable(tensor, requires_grad=True)
y = 2*variable+3

# 反向传播计算梯度
y.backward()
print(variable.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Out:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Variable containing:
2
[torch.FloatTensor of size 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;示例-计算线性模型的代价函数&#34;&gt;示例：计算线性模型的代价函数&lt;/h2&gt;

&lt;p&gt;我们来使用 Variable 来练习一下计算线性模型的代价函数，交叉熵函数为：
$$
J = - \frac{1}{m}  \sum_{i = 1}^m  \large ( \small y^{(i)} \log a^{  (i)} + (1-y^{(i)})\log (1-a^{  (i)} )\large )
$$
我们可以简单地使用内建函数来进行计算。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# torch.nn.functional中提供了大量常用的函数，例如Sigmoid，Softmax等
import torch.nn.functional as F
X = Variable(torch.Tensor([0.2,0.4,0.7,0.9]))
z = F.sigmoid(X)
y = Variable(torch.Tensor([0, 0, 1, 1]))
cost = F.binary_cross_entropy(z, y)
print(cost)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Out:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Variable containing:
0.6139
[torch.FloatTensor of size 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cuda-支持-可选&#34;&gt;CUDA 支持（可选）&lt;/h2&gt;

&lt;p&gt;如果你的电脑显卡能够支持 CUDA ，可以在 Pytorch 中使用 CUDA 来加速计算过程。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# let us run this cell only if CUDA is available
if torch.cuda.is_available():
    x = x.cuda()
    y = y.cuda()
    x + y
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>
