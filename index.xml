<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Codle on Codle</title>
    <link>https://codle.net/</link>
    <description>Recent content in Codle on Codle</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>&amp;copy; 2018 蜀ICP备17002933号-2</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>基于 React Native 的家校通 App</title>
      <link>https://codle.net/project/school2home/</link>
      <pubDate>Wed, 28 Mar 2018 16:27:27 +0800</pubDate>
      
      <guid>https://codle.net/project/school2home/</guid>
      <description>

&lt;h2 id=&#34;介绍&#34;&gt;介绍&lt;/h2&gt;

&lt;p&gt;XX&lt;/p&gt;

&lt;h2 id=&#34;项目环境&#34;&gt;项目环境&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;React Native 0.51&lt;/li&gt;
&lt;li&gt;Django 2.0&lt;/li&gt;
&lt;li&gt;Python 3.6&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;许可&#34;&gt;许可&lt;/h2&gt;

&lt;p&gt;MIT 协议&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>推荐读物</title>
      <link>https://codle.net/post/recommend/</link>
      <pubDate>Sat, 17 Mar 2018 18:06:01 +0800</pubDate>
      
      <guid>https://codle.net/post/recommend/</guid>
      <description>

&lt;p&gt;分享一些平时看过的觉得还不错的书籍、在线课程，主要是计算机专业方向的推荐读物。还在补充中，有好的推荐也可以评论留言。&lt;/p&gt;

&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;

&lt;p&gt;纯入门向推荐「简明 Python 教程」， 英文名「A Byte of Python」，是一本英文读物。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;在线阅读：&lt;a href=&#34;https://python.swaroopch.com/&#34; target=&#34;_blank&#34;&gt;A Byte of Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;廖雪峰的 Python3 教程。在他的个人博客上，不仅介绍了基本的语法也有一些使用 Python 写的有趣的小项目。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;网站地址：&lt;a href=&#34;https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000&#34; target=&#34;_blank&#34;&gt;Python3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Python 工具书籍类书籍：「Python Cookbook」，和「C++ Prime」类似，介绍了 Python 中的基本算法写法、模块写法等。内容太多，用到的时候查询即可。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;下载地址：&lt;a href=&#34;https://pan.baidu.com/s/1pL1cI9d&#34; target=&#34;_blank&#34;&gt;https://pan.baidu.com/s/1pL1cI9d&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;machine-learning&#34;&gt;Machine Learning&lt;/h2&gt;

&lt;p&gt;吴恩达在斯坦福时期教授的课程的在线版本，也是 Coursera 平台起步阶段最热门的课程。每节课都有小作业，可以在线评分。数学代码使用使用 Ocatve ，一款开源的数学软件，和 Matlab 代码可以通用，也就是说使用 Matlab 或者 Ocatve 都可以完成作业。&lt;/p&gt;

&lt;p&gt;课程相比于斯坦福的本科生版本有所删减，难度降低了，追求更高层次的可以去网易公开课看本科教室录制的&lt;a href=&#34;http://open.163.com/special/opencourse/machinelearning.html&#34; target=&#34;_blank&#34;&gt;斯坦福 cs229&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;课程地址：&lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34; target=&#34;_blank&#34;&gt;旁听免费&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;除了 Coursera 平台的机器学习课程，台湾大学的林轩田教授的两门课程也是非常值得一看的，且全部使用的国语讲述。&lt;/p&gt;

&lt;p&gt;课程地址：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/av12463015/&#34; target=&#34;_blank&#34;&gt;机器学习基石(B站源)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/av12469267&#34; target=&#34;_blank&#34;&gt;机器学习技法(B站源)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;deep-learning&#34;&gt;Deep Learning&lt;/h2&gt;

&lt;p&gt;吴恩达创业项目 deeplearning.ai 的课程，较短偏向于工程应用，很多公式都没有推导，适合入门，使用 TensorFlow 作为深度学习框架来布置作业。分为五个课程：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Neural Networks and Deep Learning&lt;/li&gt;
&lt;li&gt;Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/li&gt;
&lt;li&gt;Structuring Machine Learning Projects&lt;/li&gt;
&lt;li&gt;Convolutional Neural Networks&lt;/li&gt;
&lt;li&gt;Sequence Models&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;课程地址：&lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34; target=&#34;_blank&#34;&gt;旁听免费&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;吴恩达的课程部分字幕没有完全汉化，对英语有难度的还可以选择台湾大学李宏毅的深度学习课程，也是全程国语，但是相对于吴恩达的课程，少了很多在线练习的机会。&lt;/p&gt;

&lt;p&gt;课程地址：&lt;a href=&#34;https://www.bilibili.com/video/av9770302&#34; target=&#34;_blank&#34;&gt;深度学习(2017)(B站源)&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch 中可变长序列的处理</title>
      <link>https://codle.net/post/how-to-deal-with-variable-length-sequences-in-pytorch/</link>
      <pubDate>Fri, 16 Mar 2018 16:21:08 +0800</pubDate>
      
      <guid>https://codle.net/post/how-to-deal-with-variable-length-sequences-in-pytorch/</guid>
      <description>

&lt;p&gt;最近在做文本情感分析方面的研究，主要使用了 PyTorch 来做实验。在针对文本的处理过程中，遇见了文本长度不唯一的问题，在此主要讲 PyTorch 中的解决方法。&lt;/p&gt;

&lt;p&gt;写作时的系统环境：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;PyTorch ：0.3.1&lt;/li&gt;
&lt;li&gt;Python ：3.6.2&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;问题来源&#34;&gt;问题来源&lt;/h2&gt;

&lt;p&gt;情感分析采用神经网络的方法和文本分类比较类似。其工作流程为输入文本，输出情感类别。对比文本分类和图片分类，图片在进行处理前都会将图片缩放为尺寸相当，这样保证了输入与输出的尺寸在一开始就被确定。而文本，一般以句子为一个输入，每个词对应一个 LSTM 或者 RNN 单元，然而当多个样本进行训练时，句子长度不一给模型训练带来了极大麻烦。&lt;/p&gt;

&lt;h2 id=&#34;解决方法&#34;&gt;解决方法&lt;/h2&gt;

&lt;h3 id=&#34;解决思路&#34;&gt;解决思路&lt;/h3&gt;

&lt;p&gt;通常情况下，我们会考虑使用 padding 的方法来统一序列长度。例如，现在有两句话：“Look at the cat”和 “OK&amp;rdquo;，第一句话的长度为 4，第二句为 1，那么我们就考虑将第二句话后面补 0 使得其长度达到 4。此时，句子长度便统一了。&lt;/p&gt;

&lt;p&gt;这种方法有个明显的问题：如果训练集存在一个非常长的句子，而大部分句子都非常短小，那么整个训练集合都会补充相当数量的 0。此时再进行神经网络训练的时候，输入矩阵非常稀疏，会导致训练效果和训练效率都变得很低。&lt;/p&gt;

&lt;p&gt;为了解决这个问题，PyTorch 中采用了一种打包解包的方法。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://upload-images.jianshu.io/upload_images/2397007-93b28bb8d6948571.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如图所示，将填充后的数据进行打包（重新压缩），data 存储拼接后的字符串，batch_sizes 保存原始串的长度。其实就是采用了一个结构体，一部分存储全部的数据，一部分存储每个串的长度，然后根据串长度取回原来的序列，只是 PyTorch 中实现了许多我们可以用到的方法，而不用自己手动实现了。&lt;/p&gt;

&lt;h3 id=&#34;pytorch-实现&#34;&gt;PyTorch 实现&lt;/h3&gt;

&lt;p&gt;在 Pytorch 中，提供了两个方法&lt;code&gt;torch.nn.utils.rnn.pack_padded_sequence()&lt;/code&gt;和&lt;code&gt;torch.nn.utils.rnn.pad_packed_sequence()&lt;/code&gt; （以下简写为 &lt;code&gt;pack_padded_sequence()&lt;/code&gt; 和 &lt;code&gt;pad_packed_sequence()&lt;/code&gt; 。&lt;/p&gt;

&lt;h4 id=&#34;pack-padded-sequence-文档&#34;&gt;pack_padded_sequence 文档&lt;/h4&gt;

&lt;p&gt;原型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;打包一个填充过的可变长度序列的 Variable 。&lt;/p&gt;

&lt;p&gt;输入应该是 TxBx* ，其中 T 是最长的序列的长度（等同于lengths[0]），B 是 batch 大小，而 * 可以是任意数字的维度 （包括 0）。如果 batch_first 为 True ，那么输入就应该是 BxTx* 。&lt;/p&gt;

&lt;p&gt;序列应该被按照序列的长度呈降序排列。例如： input[:,0] 应该是最长的序列，而且 input[:,B-1] 是最短的。&lt;/p&gt;

&lt;p&gt;参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;input (Variable) – 经过填充的批量可变长度序列。&lt;/li&gt;
&lt;li&gt;lengths (list[int]) – 储存当前批次中的每个可变长度序列的长度的 list 对象。&lt;/li&gt;
&lt;li&gt;batch_first (bool, optional) – 如果为 True，输入则应该是 BxTx* 格式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;返回值:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一个 PackedSequence 对象&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;pad-packed-sequence-文档&#34;&gt;pad_packed_sequence 文档&lt;/h4&gt;

&lt;p&gt;原型：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;填充一批打包后的可变长度序列。&lt;/p&gt;

&lt;p&gt;是 pack_padded_sequence() 的逆操作。&lt;/p&gt;

&lt;p&gt;返回值 Variable 的数据格式为 TxBx* ，其中 T 是最长序列的长度， B 是 batch 大小。如果 batch_first 为 True，数据会被转为 BxTx* 格式。&lt;/p&gt;

&lt;p&gt;当前批次中的元素会被按照长度进行排序。&lt;/p&gt;

&lt;p&gt;参数:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sequence (PackedSequence) – 准备填充的批量元素。&lt;/li&gt;
&lt;li&gt;batch_first (bool, optional) – 如果为 True，输出会是 BxTx* 格式。&lt;/li&gt;
&lt;li&gt;padding_value (float, optional) – 填充的元素。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;返回值:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;填充序列的 Variable 元组以及包含当前批次中序列长度的 list 对象。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;使用方法&#34;&gt;使用方法&lt;/h3&gt;

&lt;p&gt;从文档中可以看出，当我们将数据填充整齐后，可以使用&lt;code&gt;pack_padded_sequence()&lt;/code&gt; 和&lt;code&gt;pad_packed_sequence()&lt;/code&gt; 方法可以将数据进行打包和解包（填充这一步暂且需要自己手动做）。&lt;/p&gt;

&lt;p&gt;打包后得到 PackedSequence 对象，其主要由两个部分组成：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;所有序列的数据&lt;/li&gt;
&lt;li&gt;每个序列的原始长度&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;针对 PackedSequence 对象，我们可以使用很多方法而不需要去解包（解包这一步会比较慢）。PackedSequence 对象可以被当作输入用在 PyTorch 中的大部分模块中。&lt;/p&gt;

&lt;h3 id=&#34;尚未实装的方法&#34;&gt;尚未实装的方法&lt;/h3&gt;

&lt;p&gt;本方法来源于 &lt;a href=&#34;https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/2120/24&#34; target=&#34;_blank&#34;&gt;PyTorch 官方论坛&lt;/a&gt; ，因为所用版本还未发布所以没有测试。文中指出在 PyTorch 0.4 版本中增加了新的方法来实现 Padding 和 Packing。&lt;/p&gt;

&lt;p&gt;序列打包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; import torch.nn.utils.rnn as rnn_utils
&amp;gt;&amp;gt;&amp;gt; a = torch.Tensor([1, 2, 3])
&amp;gt;&amp;gt;&amp;gt; b = torch.Tensor([4, 5])
&amp;gt;&amp;gt;&amp;gt; c = torch.Tensor([6])
&amp;gt;&amp;gt;&amp;gt; packed = rnn_utils.pack_sequence([a, b, c])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;序列解包：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import torch
&amp;gt;&amp;gt;&amp;gt; import torch.nn.utils.rnn as rnn_utils
&amp;gt;&amp;gt;&amp;gt; a = torch.Tensor([1, 2, 3])
&amp;gt;&amp;gt;&amp;gt; b = torch.Tensor([4, 5])
&amp;gt;&amp;gt;&amp;gt; c = torch.Tensor([6])
&amp;gt;&amp;gt;&amp;gt; rnn_utils.pad_sequence([a, b, c], batch_first=True)

 1  2  3
 4  5  0
 6  0  0
[torch.FloatTensor of size (3,3)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;在 PyTorch 中处理可变长度序列的基本流程：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;将序列填充，并按照原始长度降序排列；&lt;/li&gt;
&lt;li&gt;使用 &lt;code&gt;pack_padded_sequence()&lt;/code&gt; 对序列进行打包；&lt;/li&gt;
&lt;li&gt;使用 PackedSequence 对象继续编写程序。&lt;/li&gt;
&lt;li&gt;（可选）如果需要还原，则使用 &lt;code&gt;pad_packed_sequence()&lt;/code&gt; 方法。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://discuss.pytorch.org/t/simple-working-example-how-to-use-packing-for-variable-length-sequence-inputs-for-rnn/&#34; target=&#34;_blank&#34;&gt;Simple working example how to use packing for variable-length sequence inputs for rnn, &lt;em&gt;PyTorch Discuss&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/34418001&#34; target=&#34;_blank&#34;&gt;忆臻, pytorch中如何处理RNN输入变长序列padding, &lt;em&gt;知乎&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cnblogs.com/lindaxin/p/8052043.html&#34; target=&#34;_blank&#34;&gt;深度学习1, pytorch对可变长度序列的处理, &lt;em&gt;博客园&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/28472545&#34; target=&#34;_blank&#34;&gt;赵普, pytorch RNN 变长输入 padding, &lt;em&gt;知乎&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzAwNDI4ODcxNA==&amp;amp;mid=2652245658&amp;amp;idx=1&amp;amp;sn=03c63ad0e0657e5272ea2e3c7e346521&amp;amp;scene=0#wechat_redirect&#34; target=&#34;_blank&#34;&gt;Thomas Wolf, 如何用pyTorch改造基于Keras的MIT情感理解模型, &lt;em&gt;人工智能头条&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>面向中文文本的情感倾向分析</title>
      <link>https://codle.net/project/sentiment-ananalysis/</link>
      <pubDate>Fri, 16 Mar 2018 13:51:00 +0800</pubDate>
      
      <guid>https://codle.net/project/sentiment-ananalysis/</guid>
      <description>

&lt;p&gt;本项目使用 PyTorch 实现文本的情感分类，包括 LSTM 模型等，允许使用者自定义模型和词典。&lt;/p&gt;

&lt;p&gt;项目地址：6月8日后贴出。&lt;/p&gt;

&lt;h2 id=&#34;下载-安装&#34;&gt;下载&amp;amp;安装&lt;/h2&gt;

&lt;p&gt;暂无&lt;/p&gt;

&lt;h2 id=&#34;依赖项&#34;&gt;依赖项&lt;/h2&gt;

&lt;p&gt;暂无&lt;/p&gt;

&lt;h2 id=&#34;使用方法&#34;&gt;使用方法&lt;/h2&gt;

&lt;p&gt;暂无&lt;/p&gt;

&lt;h2 id=&#34;许可&#34;&gt;许可&lt;/h2&gt;

&lt;p&gt;本项目使用 MIT 协议。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PyTorch 基础入门（一）</title>
      <link>https://codle.net/post/pytorch-start-tutorial-1/</link>
      <pubDate>Mon, 11 Dec 2017 16:48:54 +0800</pubDate>
      
      <guid>https://codle.net/post/pytorch-start-tutorial-1/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;本文参考自deeplearning.ai的在线课程deep learning specialization课程中的第二项课程中的Tensorflow教程，希望能够对使用Pytorch来进行研究深度学习的同学有所帮助。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;采用 Jupyter Notebook 的方式编写本文。你可以在相关材料中找到相应的文件来尝试自己练习。通过本文，将会涉及到：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;初始化变量&lt;/li&gt;
&lt;li&gt;创建模型&lt;/li&gt;
&lt;li&gt;训练算法&lt;/li&gt;
&lt;li&gt;生成一个神经网络&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;pytorch-库&#34;&gt;Pytorch 库&lt;/h1&gt;

&lt;p&gt;首先，我们引入相关的库：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math
import numpy as np
import matplotlib.pyplot as plt
import torch

%matplotlib inline
np.random.seed(1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;导入库后，我们将在接下来的不同的示例中去学习使用他们。&lt;/p&gt;

&lt;h1 id=&#34;tensor&#34;&gt;Tensor&lt;/h1&gt;

&lt;p&gt;在一切开始之前，我们需要学习在 Pytorch 中最基本的数据单元 Tensor。如果之前对 numpy 有所使用，那么我们可以先简单的认为 Tensor 和 narray 是类似的数据类型，即多维的数据单元。&lt;/p&gt;

&lt;p&gt;Tensor 有多种创建方式，下面简单介绍几种：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 直接创建未初始化的 Tensor 变量，参数为创建的矩阵维数
X = torch.Tensor(5, 3)

# 指明为浮点型的 Tensor 变量
X_float = torch.FloatTensor(5, 3)

# 列表创建
# 一维
X_one = torch.Tensor([1, 2, 3])
# 二维
X_two = torch.Tensor([[1, 2, 3], [4, 5, 6]])

# 使用 numpy 变量进行创建
nones = np.ones((5, 3))
X_ones = torch.Tensor(nones)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;示例-损失函数&#34;&gt;示例：损失函数&lt;/h2&gt;

&lt;p&gt;我们先将计算损失函数作为本教程的第一个示例：
$$
loss = L(\hat y, y)=(\hat y^{(i)}-y^{(i)})^2
$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y_hat = torch.Tensor([1, 1, 0])
y = torch.Tensor([1, 0, 1])
loss = (y_hat - y)**2
print(loss)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Out:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0 
1 
1
[torch.FloatTensor of size 3]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;也可以使用 &lt;code&gt;torch.mean()&lt;/code&gt; 来计算均方差：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;loss = torch.mean((y_hat - y)**2)
print(loss)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Out:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;0.6666666666666666
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;variable&#34;&gt;Variable&lt;/h1&gt;

&lt;p&gt;前面主要讲了最基本的数据类型 Tensor ，这一节主要谈论 Variable 类型，这一类型主要用于存放变化的数据。如何理解 Variable 变量？在使用框架之前我们曾使用 numpy 来编写简单的神经网络，numpy 单一的数据类型已经能够满足编写神经网络了，那么为什么要用 Variable 呢？&lt;/p&gt;

&lt;p&gt;Variable 类型最为主要的功能就是计算变量导数，对于一个线性函数：
$$
y = 2x+3
$$
$x$ 和 $y$ 被称为变量（自变量和因变量）。$x$ 变化时，$y$ 随 $x$ 的变化而变化，那么当 $x$ 变化 0.1的时候， $y$ 变化多少呢，在通常情况下，我们可以通过计算导数：
$$
\frac{dy}{dx}=2,\delta y=\frac{dy}{dx}\times\delta x=2\times \delta x=2\times0.1=0.2
$$
在之前的学习中，我们自己手动计算导数，将其放入神经网络中，而在 Pytorch 框架中这一计算不再需要我们自己动手了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.autograd import Variable
tensor = torch.Tensor(1)
variable = Variable(tensor, requires_grad=True)
y = 2*variable+3

# 反向传播计算梯度
y.backward()
print(variable.grad)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Out:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Variable containing:
2
[torch.FloatTensor of size 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;示例-计算线性模型的代价函数&#34;&gt;示例：计算线性模型的代价函数&lt;/h2&gt;

&lt;p&gt;我们来使用 Variable 来练习一下计算线性模型的代价函数，交叉熵函数为：
$$
J = - \frac{1}{m}  \sum_{i = 1}^m  \large ( \small y^{(i)} \log a^{  (i)} + (1-y^{(i)})\log (1-a^{  (i)} )\large )
$$
我们可以简单地使用内建函数来进行计算。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# torch.nn.functional中提供了大量常用的函数，例如Sigmoid，Softmax等
import torch.nn.functional as F
X = Variable(torch.Tensor([0.2,0.4,0.7,0.9]))
z = F.sigmoid(X)
y = Variable(torch.Tensor([0, 0, 1, 1]))
cost = F.binary_cross_entropy(z, y)
print(cost)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Out:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Variable containing:
0.6139
[torch.FloatTensor of size 1]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;cuda-支持-可选&#34;&gt;CUDA 支持（可选）&lt;/h2&gt;

&lt;p&gt;如果你的电脑显卡能够支持 CUDA ，可以在 Pytorch 中使用 CUDA 来加速计算过程。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# let us run this cell only if CUDA is available
if torch.cuda.is_available():
    x = x.cuda()
    y = y.cuda()
    x + y
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
  </channel>
</rss>
